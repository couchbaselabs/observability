// This file is automatically generated - DO NOT EDIT IT.
// Instead, edit checker_defs.yaml and/or checkers.adoc.tmpl, then run `make generate`

= Cluster Monitor checkers
:page-partial:

== Cluster Checkers
// tag::group-cluster[]

[#CB90002]
=== Single or Two Node Cluster (CB90002)


*Background*: Couchbase recommends that all production clusters have at least three nodes.
Clusters with fewer than three nodes mean that automatic failover is not possible and the number of bucket replicas is limited to 0 or 1, leading to reduced durability.


*Condition*: Only one or two nodes detected in the cluster.

*Remediation*: Add more nodes to the cluster.

*Further Reading*: https://docs.couchbase.com/server/current/install/deployment-considerations-lt-3nodes.html[About Deploying Clusters With Less Than Three Nodes]


[#CB90004]
=== Mixed Version Cluster (CB90004)


*Background*: While Couchbase Server does support running multiple versions as part of a cluster, this is only recommended during an upgrade, rather than as a long-term state.
The cluster features available will be those of the lowest-version node.


*Condition*: Multiple nodes detected with differing Couchbase Server versions.

*Remediation*: Upgrade all nodes to the same version.
If this alert is present during an upgrade, it can safely be disregarded until the upgrade is complete.

*Further Reading*: https://docs.couchbase.com/server/current/install/upgrade-feature-availability.html[Feature Availability during Upgrade],
https://docs.couchbase.com/server/current/install/upgrade.html[Upgrade]


[#CB90006]
=== Auto Compaction Enabled (CB90006)


*Background*: Couchbase Server uses an append-only store on disk to ensure data durability.
This needs to be compacted periodically, otherwise performance can be degraded or the disk can fill up.
This is done automatically by default.


*Condition*: No auto-compaction threshold set.

*Remediation*: Enable auto-compaction in the cluster settings

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/storage.html#append-only-writes-and-auto-compaction[Storage]


[#CB90007]
=== Auto-Failover Enabled (CB90007)


*Background*: Couchbase Server can automatically fail over dead or unhealthy nodes, to ensure continuity of cluster operations.
If auto-failover is disabled, node failure will result in some requests being unable to be serviced.


*Condition*: Auto-failover is disabled.

*Remediation*: Adjust auto-failover settings

*Further Reading*: https://docs.couchbase.com/server/current/learn/clusters-and-availability/automatic-failover.html[Automatic Failover]


[#CB90008]
=== Number of Buckets (CB90008)


*Background*: Couchbase Server supports up to 30 buckets in a cluster as of version 6.5.
Going above this number may cause performance degradation.


*Condition*: More than 30 buckets in the cluster.

*Remediation*: Reduce the number of buckets in the cluster.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/buckets.html[Buckets]


[#CB90011]
=== Data Loss Messages (CB90011)


*Background*: If a node is failed over, and the active vBuckets stored on it have no replicas, data loss can result.
Note that this is only possible if manually performing a hard failover, as Couchbase Server will never perform an automatic failover if there is a risk of data loss.


*Condition*: Messages indicating data loss due to failover detected in the cluster logs.

*Remediation*: Contact Couchbase Technical Support _immediately_.
Isolate the failed-over node, and do not make _any_ changes to its configuration or attempt to recover it unless instructed by Couchbase Technical Support.
There is a risk of permanent, irrecoverable data loss.

*Further Reading*: https://docs.couchbase.com/server/current/learn/clusters-and-availability/hard-failover.html[Hard Failover]


[#CB90016]
=== All Nodes are Active (CB90016)


*Background*: The Couchbase Cluster Manager periodically contacts all nodes in the cluster to check their status.
If a node fails to respond, it is marked as inactive.


*Condition*: One or more inactive nodes are present.

*Remediation*: Rebalance the unhealthy nodes out of the cluster, and replace them if appropriate.
Examine the other health check results to identify the potential cause, or contact Couchbase Technical Support for a root cause analysis.

*Further Reading*: https://docs.couchbase.com/server/current/learn/clusters-and-availability/clusters-and-availability.html[Clusters and Availability],
https://docs.couchbase.com/server/current/learn/clusters-and-availability/failover.html#detecting-node-failure[Detecting Node Failure]


[#CB90019]
=== Asymmetrical Cluster (CB90019)


*Background*: Couchbase recommends that all nodes in the cluster have identical hardware.
Since clients may access any node in the cluster to service a request, differing hardware can lead to unpredictable application performance.


*Condition*: Nodes with differing amounts of CPUs or RAM detected.

*Remediation*: Ensure all nodes have identical hardware.

*Further Reading*: https://docs.couchbase.com/server/current/install/sizing-general.html[Sizing Guidelines]


[#CB90020]
=== Bucket Count Checks (CB90020)


*Background*: Couchbase recommends that there are at least as many CPUs on each node as there are buckets.
If fewer CPUs are available, the buckets will compete with each other for resources, potentially causing degraded performance.
If this is not possible due to resource constraints, Couchbase recommends that each bucket have at least 0.4 CPU cores on Couchbase Server 6.x and below, and 0.2 on 7.x.
Additionally, no more than 10 (on versions below 6.5) or 30 (on 6.5 and later) total buckets are supported.
When these limits are reached, the cluster can get failures across nodes.


*Condition*: Fewer CPUs than buckets detected on the node.
Less than or equal to 0.4 cores/bucket on 6.x and less than or equal to 0.2 cores/bucket on 7.x.
More than 30 buckets in the cluster.

*Remediation*: Upgrade the nodes' hardware or reduce the number of buckets.

*Further Reading*: https://docs.couchbase.com/server/current/install/sizing-general.html[Sizing Guidelines]


[#CB90022]
=== Backup Location Check (CB90022)


*Background*: If the Couchbase Backup Service cannot access the backup archive location, backup failures may result, leading to reduced durability.

*Relevant To Versions*:
7.0.0 and above
*Condition*: The number of backup location errors has increased in the past three days.

*Remediation*: Ensure the Backup Service has consistent access to its archive location.

*Further Reading*: https://docs.couchbase.com/server/current/learn/services-and-indexes/services/backup-service.html[Backup Service]


[#CB90023]
=== Orphaned Backup Tasks (CB90023)


*Background*: An "orphaned" backup task is a task that is marked as running, but no node is actually executing it.
This can happen if that node cannot for some reason send a status report to the Backup Service leader (for example it suffered a power cut or a network outage).
These may be transient errors, but seeing a consistent increase in the number of orphaned tasks can indicate a problem with the Backup Service.

*Relevant To Versions*:
7.0.0 and above
*Condition*: The number of orphaned backup tasks has increased in the past three days.

*Remediation*: Review the Backup Service logs to identify the cause of the problem, or contact Couchbase Technical Support.

*Further Reading*: https://docs.couchbase.com/server/current/learn/services-and-indexes/services/backup-service.html[Backup Service]


[#CB90027]
=== Index Service Log Level (CB90027)


*Background*: While the log level of the Index Service can be configured, only the default setting of `Info` is supported.
Higher levels can mean valuable information is missing from the logs, while lower levels can mean the logs are rotated more frequently.
Both of these can make it difficult to diagnose issues with the Index Service.


*Condition*: Index Service log level is set to a non-default value.

*Remediation*: Change the log level to `Info`.

*Further Reading*: https://docs.couchbase.com/server/current/manage/manage-settings/general-settings.html#index-settings-via-rest[Index Settings]


[#CB90030]
=== Index With No Redundancy (CB90030)


*Background*: By default, a Global Secondary Index is only situated on one Index Service node, meaning that if that node is failed over for any reason, queries using that index will either use a primary index (causing severely degraded performance) or start failing completely.
In production use cases we always recommend indexes have either replicas or equivalent indexes (indexes with a different name but the same definition).


*Condition*: An index with no replicas or equivalent indexes is detected.

*Remediation*: Either increase the number of replicas or add equivalent indexes.

*Further Reading*: https://docs.couchbase.com/server/current/learn/services-and-indexes/indexes/index-replication.html[Index Availability and Performance]


[#CB90031]
=== Bad Redundant Index (CB90031)


*Background*: When using index replicas, the Index Service will place replicas on different nodes to ensure their availability in the event of a node failover.
However equivalent indexes do not have this protection, and it is possible to place two or more equivalent indexes on the same node.
This provides effectively no redundancy, as should that node be failed over all the equivalent indexes will be lost and queries may start failing or experience severely degraded performance.


*Condition*: Multiple equivalent indexes on the same node.

*Remediation*: Move the indexes to different Index Service nodes.
Consider using index replicas instead.

*Further Reading*: https://docs.couchbase.com/server/current/learn/services-and-indexes/indexes/index-replication.html[Index Availability and Performance]


[#CB90032]
=== Too Many Index Replicas (CB90032)


*Background*: After an index node is failed over, it is possible that an index has more replicas than there are Index Service nodes.
This does not provide the desired level of redundancy and durability.


*Condition*: Index with more replicas than there are Index Service nodes.

*Remediation*: Either reduce the number of replicas, or add more Index Service nodes.


// Checker CB90033 is documented in observability


[#CB90035]
=== Empty Server Group (CB90035)


*Background*: There is no practical use for having an empty server group, so if one is present it is most likely a mistake.


*Condition*: One or more server groups exist that do not contain any nodes.

*Remediation*: Remove the empty server group.

*Further Reading*: https://docs.couchbase.com/server/current/manage/manage-groups/manage-groups.html[Manage Server Groups]


[#CB90059]
=== Developer Preview (CB90059)


*Background*: Developer Preview provides early access to features which may become generally available (“GA”) in future releases and enables you to experiment with these features to get a sense of how they work.
However, this mode is unsupported, so it should not be used in production.


*Condition*: Cluster is in Developer Preview mode.

*Remediation*: If this is a development only cluster, you do not need to do anything, otherwise create a new cluster that is not in Developer Preview mode.

*Further Reading*: https://docs.couchbase.com/server/current/developer-preview/preview-mode.html[Developer Preview Mode]


[#CB90063]
=== Duplicate Node UUID Check (CB90063)


*Background*: Couchbase expects the node UUID to uniquely identify each node for Cluster Manager purposes.
If this condition is not met, serious issues with rebalances and other operations may be experienced.


*Condition*: At least one node UUID is not unique in the cluster.

*Remediation*: Contact Couchbase Technical support.

*Further Reading*: https://issues.couchbase.com/browse/MB-17132[MB-17132]


[#CB90065]
=== Too many FTS Index replicas (CB90065)


*Background*: If there are more replicas configured than FTS nodes, these replicas cannot be distributed properly and may cause rebalance issues.


*Condition*: The number of FTS replicas configured is greater than or equal to the number of nodes running the Search service.

*Remediation*: Ensure there are strictly fewer FTS index replicas than nodes running the Search Service.

*Further Reading*: xref:7.0@server:fts:fts-index-replicas[FTS Replicas]


[#CB90068]
=== Missing Index Partition (CB90068)


*Background*: If a index is missing index partitions, it can cause queries that use this index to fail which can lead to client errors.


*Condition*: If the number of index partitions present is less than what was originally defined when making the index.

*Remediation*: Check if a node has been failed over.
If this is not the case, recreate the index again and contact Couchbase Technical Support.

*Further Reading*: xref:7.0@server:n1ql:n1ql-language-reference:index-partitioning[Index Partitioning]


[#CB90069]
=== Imbalanced Index Partition (CB90069)


*Background*: If an index partition is hashed on an invalid field, it results in one partition being larger than the partitions on other Index Service nodes.
This means a large chunk of a node's memory will be used by the Index Service which can then cause the `indexer` process to be killed by the Linux OOM killer.


*Condition*: An Index Service node contains an index partition which is 20% larger than partitions for the same index on other nodes.

*Remediation*: Recreate imbalanced index to redistribute index partition data, making sure the index partitions are hashed to valid fields.

*Further Reading*: https://docs.couchbase.com/server/current/n1ql/n1ql-language-reference/index-partitioning.html[Index Partitioning]


[#CB90079]
=== Default VBucket Count (CB90079)


*Background*: The vBucket count set in cluster configuration, if changed from default value (64 on macOS, 1024 on Windows/Linux) can cause failures across nodes and services.
This may impact cluster integrity and is not recommended in production environment.


*Condition*: Non-default vBucket number

*Remediation*: Set the vBucket number back to the default value (64 on macOS, 1024 on Windows/Linux).
// end::group-cluster[]
== Node Checkers
// tag::group-node[]

[#CB90001]
=== One Service Per Node (CB90001)


*Background*: Couchbase recommends that only one Multi-Dimensional Scaling service per node is run in production.
Colocating services increases the overall resource requirements of the nodes and can cause resource contention, leading to degraded performance.


*Condition*: More than one service detected per node.

*Remediation*: Move services to their own dedicated nodes.

*Further Reading*: https://docs.couchbase.com/server/current/install/sizing-general.html[Sizing Guidelines]


[#CB90003]
=== Unhealthy or Inactive Node (CB90003)


*Background*: If the Cluster Manager detects that a node is unhealthy, it will mark it as such and fail it over (if Auto-Failover is enabled and the conditions are met).
This will mean the cluster is in a degraded state with reduced durability and capacity.


*Condition*: One or more nodes are reported as unhealthy by the Couchbase Cluster Manager.

*Remediation*: Rebalance the unhealthy nodes out of the cluster, and replace them if appropriate.
Examine the other health check results to identify the potential cause, or contact Couchbase Technical Support for a root cause analysis.

*Further Reading*: https://docs.couchbase.com/server/current/learn/clusters-and-availability/clusters-and-availability.html[Clusters and Availability],
https://docs.couchbase.com/server/current/learn/clusters-and-availability/failover.html#detecting-node-failure[Detecting Node Failure]


[#CB90005]
=== Server Quota (CB90005)


*Background*: Each Couchbase Server node has a memory quota, which limits how much memory it is allowed to use.
We recommend that this is set no higher than 80-90% of the host's memory, otherwise the operating system may not have enough memory remaining to function.


*Condition*: Memory allocated to Couchbase Server nodes is greater than 80% of the hosts' memory.

*Remediation*: Increase the amount of memory on the nodes, or reduce the Couchbase Server memory quota.

*Further Reading*: https://docs.couchbase.com/server/current/install/sizing-general.html[Sizing Guidelines]


[#CB90012]
=== Server Version Supportability (CB90012)


*Background*: Couchbase Server versions are only supported for a period of time as defined in the Enterprise Software Support Policy.
Outside this period, limited or no support can be provided by Couchbase Technical Support.
We recommend you always run a supported version of Couchbase Server to take advantage of your Enterprise Support agreement.


*Condition*: Nodes running unsupported versions of Couchbase Server detected.

*Remediation*: Upgrade the nodes in question to a supported version of Couchbase Server. If this is not possible, contact your Couchbase Account Manager.

*Further Reading*: https://www.couchbase.com/support-policy/enterprise-software[Couchbase Enterprise Software Support Policy], https://docs.couchbase.com/server/current/install/upgrade.html[Upgrading Couchbase Server]


[#CB90014]
=== Generally Available Build (CB90014)


*Background*: Only generally available, officially released builds of Couchbase Server are supported, unless you have a specific agreement with Couchbase to use a non-GA build in production.


*Condition*: Node running non-GA build of Couchbase Server detected.

*Remediation*: Upgrade the node to a generally available build of Couchbase Server.
If you have a specific agreement with Couchbase to operate a non-GA build (for example a Maintenance Patch), it is safe to disregard this warning.


[#CB90018]
=== Node Swap Usage (CB90018)


*Background*: Couchbase Server should always have sufficient RAM available without needing to use swap space.
Couchbase Server can manage its own disk storage using ejection, so its memory being in swap can negatively affect performance.


*Condition*: Node swap usage above zero.
Upgraded to an alert if swap usage is above 90% of available swap memory.

*Remediation*: Increase available RAM on the nodes.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/memory.html[Memory]


[#CB90021]
=== Disk Space Usage (CB90021)


*Background*: Couchbase Server nodes should always have sufficient disk space to store all data.
If a node runs out of storage, it will stop accepting writes and may potentially be automatically failed over.


*Condition*: Over 90% disk usage on the node.

*Remediation*: Increase the amount of disk space available.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/storage.html[Storage]


[#CB90025]
=== Transparent Huge Pages (CB90025)


*Background*: The Linux kernel supports _transparent huge pages_ (THP), a feature that reduces memory management overhead.
Although it is often beneficial for general purpose workloads, it can cause performance degradation for databases like Couchbase Server.
Therefore, we recommend disabling THP.


*Condition*: Transparent Huge Pages set to `always`.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Set the THP configuration to `madvise` or `never`.

*Further Reading*: https://docs.couchbase.com/server/current/install/thp-disable.html[Disabling Transparent Huge Pages]


[#CB90026]
=== Service Status (CB90026)


*Background*: Couchbase Server uses a number of ports to communicate between clients and services.
If these are blocked by a firewall, this can cause connection failures for clients or other cluster problems.


*Condition*: Cluster Monitor cannot communicate with the node on the specified ports.

*Remediation*: Ensure there is no firewall blocking communication. Review your infrastructure for networking issues.

*Further Reading*: https://docs.couchbase.com/server/current/install/install-ports.html[Couchbase Server Ports]


[#CB90028]
=== Services Sharing File Systems (CB90028)




[#CB90034]
=== Below Minimum Node Memory (CB90034)


*Background*: The recommended minimum memory for each node in your Couchbase Server cluster to have is 4 Gigabytes.
Any less than this and Couchbase Server could display unwanted behaviour.


*Condition*: A node has less than 4GB of RAM.

*Remediation*: Upgrade the node's hardware.

*Further Reading*: https://docs.couchbase.com/server/current/install/pre-install.html[System Resource Requirements]


[#CB90040]
=== Supported/Deprecated OS (CB90040)


*Background*: Each version of Couchbase Server supports certain operating systems.
Using unsupported OS versions may cause various issues, including Couchbase Server or its services failing to start, and may render your cluster unsupportable.


*Condition*: A node has an operating system version not supported for the version of Couchbase Server in use.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Upgrade the operating system of the node.

*Further Reading*: https://docs.couchbase.com/server/current/install/install-platforms.html[Supported Operating Systems]


[#CB90042]
=== Segmentation Faults (CB90042)


*Background*: A segmentation fault (segfault) occurs when a process reads invalid or restricted memory.
Segmentation faults are nearly always a bug, and often cause processes to crash, leading to degraded availability and system instability.


*Condition*: Segmentation faults seen in the system logs.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Examine the system logs.
If a Couchbase process was the one to crash, contact Couchbase Technical Support.


[#CB90044]
=== Managed Service Crash (CB90044)


*Background*: The "babysitter" is part of Couchbase Server's cluster manager which is responsible for maintaining a variety of Couchbase Server processes.
If any of the processes managed by the babysitter die, it is responsible for restarting them.


*Condition*: A process managed by babysitter crashes.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: A process can crash for a number of reasons, so if it happens once or twice it is not indicative of a Couchbase Server issue.
However, if it is happening repeatedly or you do notice disruption in your cluster please contact Couchbase Technical Support.

*Further Reading*: https://docs.couchbase.com/server/current/learn/clusters-and-availability/cluster-manager.html[Cluster Manager]


[#CB90045]
=== Used Memory Percentage Check (CB90045)


*Background*: If more than 90% of RAM is in use then Couchbase Server performance may be negatively affected.
This is because there needs to be enough RAM for the operating system and to avoid swapping.


*Condition*: More than 90% of available RAM is used.

*Remediation*: Add more RAM to the node, or review the resource usage of other applications on the server.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/memory.html[Service Memory Quotas],
https://docs.couchbase.com/server/current/install/sizing-general.html[Sizing Guidelines]


[#CB90058]
=== Open File / Process Limits (CB90058)


*Background*: Linux processes have a limit of how many file descriptors (files, network sockets, etc.) can be open at a time, and how many processes a user can create.
These limits are in place to prevent issues such as fork bombs, but the default values are often too low on many distros.
Exceeding these limits can cause hard-to-diagnose issues, including Couchbase Server failing to start.

You can verify the values of the limits using the `ulimit -n` and `ulimit -u` commands respectively.


*Condition*: Open file / process limits for the Couchbase Server `babysitter` process are below the recommended value.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Increase the open file / process limit for the Couchbase Server processes.

*Further Reading*: https://docs.couchbase.com/server/current/install/rhel-suse-install-intro.html#setting-max-process-limits[Setting Max Process Limits],
https://docs.couchbase.com/server/current/install/non-root.html#establish-limits-for-user-processes-and-file-descriptors[Establish Limits for User Processes and File Descriptors]


[#CB90060]
=== Out-Of-Memory Killer Activity (CB90060)


*Background*: Linux will engage the Out-Of-Memory (OOM) Killer when the system is critically low on available RAM.
Since the OOM killer will kill the fewest possible processes to reclaim as much memory as possible, and since Couchbase Server processes generally use a lot of memory, they are often the first to be killed.

Even if Couchbase Server processes are not themselves killed, OOM killer activity is generally a sign that the node may be underprovisioned.


*Condition*: OOM kill messages are seen in the kernel log (`dmesg`).
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Review available memory on the node.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/memory.html[Memory],
https://docs.couchbase.com/server/current/install/sizing-general.html[Sizing Guidelines]


[#CB90064]
=== Node-to-Node Communication Issues (CB90064)


*Background*: Couchbase Server requires a number of ports to be open between all nodes in the cluster.
If these ports are not open, it can cause various problems as the services cannot communicate with each other.

Note that this list of ports is different to the ports needed for application clients to communicate with the cluster.


*Condition*: A node detects that it cannot establish TCP connections to another node.

[NOTE]
====
Not all internal ports are currently checked, so there may still be intra-cluster communication issues even if this health check is good.
You should ensure that all ports on the below page are unblocked between all nodes.
====
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Verify the ports listed in the alert, and ensure there are no firewalls or other network configuration issues between the listed nodes.

*Further Reading*: https://docs.couchbase.com/server/current/install/install-ports.html[Couchbase Server Ports]


// Checker CB90072 is documented in observability


[#CB90074]
=== SYN Flooding (CB90074)


*Background*: SYN packets are normally generated when a client attempts to start a TCP connection to a node.
SYN flooding occurs when the buffer used to store SYN packets becomes full.
This can be a result of the node not being able to keep up with the rate of incoming connections, which may be because of a Denial of Service attack.


*Condition*: SYN flooding message detected in `dmesg`.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Reduce the number of incoming connections to specified port.

*Further Reading*: https://docs.couchbase.com/server/current/rest-api/rest-manage-cluster-connections.html[Manage Cluster Connections]


[#CB90075]
=== CPU Soft Lockup (CB90075)


*Background*: Soft lockup is a symptom of a task/kernel thread using and not releasing CPU for a period of time.
It can usually occur as a kernel bug or when deploying Couchbase Server in an overcommitted Virtual Environment.


*Condition*: Soft lockup message detected in Linux 'dmesg'.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: If deploying Couchbase Server in a Virtual Environment check if said enviroment is overcommitted.

*Further Reading*: https://docs.couchbase.com/server/current/install/best-practices-vm.html[Deployment Considerations for Virtual Machines and Containers]


[#CB90076]
=== Connection Tracking Table Full (CB90076)


*Background*: If The connection tracking table (`conntrack`) becomes full, packets may be lost and clients might start timing out.
The connection table being full can be a sign that clients are not properly closing connections to Couchbase Server.


*Condition*: Connection table full message found in `dmesg`.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Check your clients are closing connections to Couchbase Server properly.


[#CB90082]
=== Couchbase Ports Status Check (CB90082)


*Background*: If the couchbase ports used by different couchbase processes are allocated to other processes then it can cause bind failures.
Also couchbase ports which are reserved for future use must be left alone, or can cause failures in upgrades.


*Condition*: Couchbase port is used by another process.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Allocate another port not used by couchbase to the other process.


[#CB90083]
=== Auto Failover Limit for VM (CB90083)


*Background*: Due to the fact that the VM is running on top of a hypervisor or container engine, there will be a minor CPU performance overhead.
In certain circumstances, the default auto-failover timeout in Couchbase can cause some issues.
It is recommended that you change the threshold from 30 seconds (the default), to 45, or even 60 seconds, depending on how CPU-intensive your workload is.


*Condition*: Auto-failover on virtual machine is set under 30 seconds.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Adjust auto-failover timeout to 45 or above, depending on your workload.

*Further Reading*: https://docs.couchbase.com/server/current/install/best-practices-vm.html[Auto-Failover Threshold]


[#CB90084]
=== Possible shared storage (CB90084)


*Background*: Couchbase Server ensures high availability by removing single points of failure from the deployment.
If a shared storage layer (such as SAN) is used in the deployment then issues with this part of the stack could lead to cluster-wide adverse effects.


*Condition*: Dmesg output indicates that a shared storage device layer might be present in the cluster.
(Requires the Couchbase Health Agent to be installed.)

*Remediation*: Ensure each node has an independent storage layer to remove a single point of failure.


[#CB90085]
=== Check High Prometheus Load Time (CB90085)


*Background*: Starting from Couchbase Server 7.0, An instance of Prometheus runs on each node of the cluster; and the metrics for each node are duly stored in that node’s instance of Prometheus.
It has been observed that lack/delay of reverse DNS resolution on the node can affect the Configuration load time of Prometheus.


*Condition*: Prometheus reporting "Completed loading of configuration file" with more than 1s totalDuration.

*Remediation*: Validate reverse DNS lookup for all the IP addresses associated with the node's hostname.


[#CB90086]
=== Document Size Too Big (CB90086)


*Background*: If a document is 19.5MB and metadata is 1MB, then the document is more than 20MB and registers DCP stream errors.


*Condition*: The document with metadata exceeds 20MB limit.

*Remediation*: Reduce document size.


[#CB90088]
=== Analytics JRE Check (CB90088)


*Background*: The Analytics Service requires a Java Runtime Environment to be installed. Only HotSpot-based JVMs, which includes the ones provided by OpenJDK and Oracle's JDK, are supported.


*Condition*: The Java Runtime Environment is not supported.

*Remediation*: Install a Java Runtime Environment version provided by a supported vendor.

*Further Reading*: https://docs.couchbase.com/server/current/install/install-environments.html


[#CB90089]
=== XDCR Invalid Datatype (CB90089)


*Background*: If the user has improperly configuration, XDCR won't go through and give EINVAL error.
Follows by SET_WITH_META errors or 403 permission error.


*Condition*: Improperly configured RBAC user.

*Remediation*: Configure RBAC User properly.

*Further Reading*: https://docs.couchbase.com/server/current/rest-api/rbac.html[RBAC]
// end::group-node[]
== Bucket Checkers
// tag::group-bucket[]

[#CB90009]
=== Missing Active vBuckets (CB90009)


*Background*: Couchbase Server buckets are sharded into a number of vBuckets, which are distributed among the nodes in the cluster.
This check verifies that all vBuckets in the cluster are in the correct state.


*Condition*: Buckets reported missing by the Cluster Manager.

*Remediation*: Rebalance the cluster, adding new nodes as necessary.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/vbuckets.html[vBuckets]


[#CB90010]
=== Missing Replica vBuckets (CB90010)


*Background*: Couchbase Server buckets are sharded into a number of vBuckets, which are distributed among the nodes in the cluster.
This check verifies that all vBuckets in the cluster are in the correct state.


*Condition*: Buckets reported missing by the Cluster Manager.

*Remediation*: Rebalance the cluster, adding new nodes as necessary.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/vbuckets.html[vBuckets]


[#CB90013]
=== Resident Ratio (CB90013)


*Background*: The resident ratio of a bucket is the percentage of its data that is stored in RAM.
Low resident ratio values may be an indication of insufficient resource allocation to the cluster.
However, they may not directly indicate a problem.


*Condition*: Resident ratio below 10%.
Upgraded to an alert if it is below 5%.

*Remediation*: Increase the bucket's memory quota.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/memory.html[Memory]


[#CB90015]
=== Number of Nodes for Replication (CB90015)


*Background*: Depending on the requested number of replica vBuckets, a certain number of Couchbase Server nodes are recommended - 5 or more for 2 replicas, or 10 or more for 3 replicas.
While it is possible to use 2 or 3 replicas with fewer nodes, this can cause performance degradation.


*Condition*: Insufficient nodes present to support the requested number of replicas.

*Remediation*: Add more nodes to the cluster, or reduce the number of replicas.

*Further Reading*: https://docs.couchbase.com/server/current/install/sizing-general.html[Sizing Guidelines]


[#CB90017]
=== Bucket Memory Usage (CB90017)


*Background*: If a bucket's memory usage crosses the high water mark, ejection will be triggered.
By default, the high water mark is set to 85% of the bucket's quota.
If the bucket's memory usage exceeds this for a long period of time, it is possible that not enough data can be ejected to bring it down below the low water mark, and there is a risk of an out-of-memory condition.


*Condition*: The bucket's memory usage is at or above 95% of its quota for more than 5 seconds.

*Remediation*: Increase the bucket's memory quota.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/memory.html[Memory]


[#CB90024]
=== DCP active writes (CB90024)


*Background*: known bug, https://issues.couchbase.com/browse/MB-46482[MB-46482], can manifest itself as DCP replications pausing.
This can result in slow replication or rejected writes.

*Relevant To Versions*:
Between 6.5.0 and 6.6.2 (inclusive)

*Condition*: Warns if the size of synchronous writes accepted is higher than the maximum DCP buffer. Upgraded to an alert if the DCP replication is paused.

*Remediation*: Upgrade to Couchbase Server 6.6.3. If this is not viable, contact Couchbase Technical Support.

*Further Reading*: https://issues.couchbase.com/browse/MB-46482[MB-46482]


[#CB90029]
=== Large Checkpoints (CB90029)


*Background*: Checkpoints are a feature of the Database Change Protocol (DCP) to avoid needing to re-stream large amounts of data.
Large checkpoints can indicate issues with the Data Service, potentially necessitating a Couchbase Server upgrade to a version where these are resolved.


*Condition*: vBucket checkpoints are larger than either 50Mb or 1% of the bucket quota.

*Remediation*: Contact Couchbase Technical Support for analysis.

*Further Reading*: https://docs.couchbase.com/server/current/cli/cbstats/cbstats-checkpoint.html[checkpoint]


[#CB90039]
=== Memcached Fragmentation Check (CB90039)


*Background*: When the memcached heap gets fragmented, all fragmented memory becomes irretrievable and cannot be returned to the OS.
If memory keeps getting fragmented for an extended period of time then the amount of usable memory becomes limited.


*Condition*: Over 15% of the memcached heap is fragmented.

*Remediation*: Contact Couchbase Technical Support for analysis.

*Further Reading*: https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/memory.html[Memory]


[#CB90053]
=== Unknown Storage Engine Check (CB90053)


*Background*: If a bucket uses a storage engine other than "couchstore", ephemeral", or "magma", it is registered as 'Unknown'.


*Condition*: The bucket uses an unknown storage engine.

*Remediation*: Contact Couchbase Technical Support for analysis.


[#CB90077]
=== Timing Histogram Underflow (MB-40967) (CB90077)


*Background*: A known issue, https://issues.couchbase.com/browse/MB-40967[MB-40967^] affecting Couchbase Server versions between 6.5.0 and 6.6.0 inclusive, can cause _command timing histograms_ (which track how long Data Service operations take) to no longer return any data once 2.1 billion operations have been executed.
This means that there will no longer be any data on how long operations take, which may make it more difficult to diagnose Couchbase Server performance issues.

This issue is fixed in version 6.6.1.


*Condition*: Informational if a susceptible version is in use.
Upgraded to a warning if the threshold is breached or exceeded for GET or SET operations.

*Remediation*: Upgrade to Couchbase Server 6.6.1 or later.
If this is not feasible, you can use https://docs.couchbase.com/server/6.6/cli/cbstats/cbstats-reset.html[`cbstats reset`] to reset these histograms, however the issue will reoccur once 2.1 billion operations are performed again.

*Further Reading*: https://issues.couchbase.com/browse/MB-40967[MB-40967^]


[#CB90078]
=== Max TTL of Bucket (CB90078)


*Background*: For versions 5.5.x and <6.0.4, the max TTL for an item in a bucket is applied incorrectly if it exceeds 30 days.
Instead of the max TTL being applied as an offset from the current time, it is instead applied as an offset from when memcached started.
This will cause all of the documents inside the bucket to expire at the same time.

This issue is fixed in version 6.0.4.

*Relevant To Versions*:
Between 5.5.0 and 6.0.3 (inclusive)

*Condition*: Maximum TTL equals or exceeds 30 days on a susceptible version.

*Remediation*: Upgrade to Couchbase Server 6.0.4 or later. If not feasible at the moment, use absolute time if the TTL exceeds 30 days.

*Further Reading*: https://issues.couchbase.com/browse/MB-37643[MB-37643]


[#CB90081]
=== Nodes for Bucket (CB90081)


*Background*: The Data Service nodes on a cluster may not host a particular bucket due to rebalance issues or a node not being accessible.
If a bucket is not present on all data nodes then it would introduce bias among the buckets and will be problematic as biased nodes will have to serve more requests for the bucket.
This may cause slow requests and compromise cluster integrity.


*Condition*: One or more buckets not present on every data service node in the cluster.

*Remediation*: Verify all nodes are online, and rebalance if necessary.
If this still persists, contact Couchbase Technical Support.
// end::group-bucket[]
