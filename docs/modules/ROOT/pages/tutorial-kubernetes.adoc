= CMOS Kubernetes Deployment

[abstract]
A simple working example from scratch in kubernetes cluster with the https://docs.couchbase.com/operator/current/overview.html[Couchbase Autonomous Operator (CAO)^] deploying Couchbase Server then being monitored by CMOS.

include::partial$tutorial.adoc[]

== Overview

CMOS is a simple, out-of-the-box solution based on industry standard tooling to observe the state of your Couchbase cluster.

== Deploy CMOS

=== kube-state-metrics

A prerequisite to configuring CMOS is to install kube-state-metrics. This can be done via helm.

[source,console]
----
$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
$ helm repo update
$ helm upgrade --install kube-state-metrics prometheus-community/kube-state-metrics
----

=== Prometheus

We provide Prometheus configuration via a Kubernetes `ConfigMap` which contains all the details for credentials, scrape targets, etc.

[NOTE]
====
Before creating a new config, first delete the existing configmap `prometheus-config` if exists. 
====

[source,console]
----
$ kubectl delete configmap prometheus-config
----

Now create config with the below contents. Save the below content in a file called `prometheus-k8s.yml`

[source,yaml]

----
# This is a template file we use so we can substitute environment variables at launch
global:
    scrape_interval: 30s
    evaluation_interval: 30s
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
    external_labels:
        monitor: couchbase-observability-stack

# Load and evaluate rules in this file every 'evaluation_interval' seconds.
rule_files:
  # All Couchbase default rules go here
    - /etc/prometheus/alerting/couchbase/*.yaml
    - /etc/prometheus/alerting/couchbase/*.yml
  # All custom rules can go here: relative to this file
    - alerting/*.yaml
    - alerting/*.yml

alerting:
    alertmanagers:
        - scheme: http
    # tls_config:
    #   ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    # bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          path_prefix: /alertmanager/
    # Assumption is we always have AlertManager with Prometheus
          static_configs:
              - targets:
                    - localhost:9093
    # Discover alert manager instances using K8s service discovery
    # kubernetes_sd_configs:
    #   - role: pod
    # relabel_configs:
    # - source_labels: [__meta_kubernetes_namespace]
    #   regex: monitoring
    #   action: keep
    # - source_labels: [__meta_kubernetes_pod_label_app]
    #   regex: prometheus
    #   action: keep
    # - source_labels: [__meta_kubernetes_pod_label_component]
    #   regex: alertmanager
    #   action: keep
    # - source_labels: [__meta_kubernetes_pod_container_port_number]
    #   regex:
    #   action: drop

scrape_configs:
    - job_name: prometheus
      metrics_path: /prometheus/metrics
      static_configs:
          - targets: [localhost:9090]

    - job_name: couchbase-grafana
      file_sd_configs:
          - files:
                - /etc/prometheus/couchbase/monitoring/*.json
            refresh_interval: 30s

  # TODO: add unauthenticated endpoint
    - job_name: couchbase-cluster-monitor
      basic_auth:
          username: admin
          password: password
      metrics_path: /api/v1/_prometheus
    # For basic auth we cannot use file_sd
      static_configs:
          - targets: [localhost:7196]

  # Used for kubernetes deployment as we can discover the end points to scrape from the API
    - job_name: couchbase-kubernetes-pods
      kubernetes_sd_configs:
          - role: pod
      relabel_configs:
      # Scrape pods labelled with app=couchbase and then only port 9091 (exporter) or 2020 (fluent bit)
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: couchbase
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            action: keep
            regex: (9091|2020)
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

  # Kube-state-metrics default service to scrape
    - job_name: kube-state-metrics
      static_configs:
          - targets: [kube-state-metrics:8080]
----

Currently unfortunately you still need to configure the cluster monitor manually with the pod details.

Run the below command to create the prometheus config.

[source,console]

----
kubectl create configmap prometheus-config --from-file="prometheus-k8s.yml"
----

=== Observability Stack

==== RBAC

RBAC set up is required to allow CMOS to function correctly, this is currently all handled for you via the example YAML file. Save the below content in `rbac.yaml`.

[source, yaml]
----
# Prometheus service discovery via the K8S API requires some permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
    name: monitoring-endpoints-role # <.>
    labels:
        rbac.couchbase.observability.com/aggregate-to-monitoring: 'true'
rules:
    - apiGroups: ['']
      resources: [services, endpoints, pods, secrets] # <.>
      verbs: [get, list, watch]
    - apiGroups: [couchbase.com]
      resources: [couchbaseclusters]
      verbs: [get, list, watch]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
    name: monitoring-role-binding # <.>
roleRef:
    kind: ClusterRole
    name: monitoring-endpoints-role
    apiGroup: rbac.authorization.k8s.io
subjects:
    - kind: Group
      name: system:serviceaccounts
      apiGroup: rbac.authorization.k8s.io
----

<.> ClusterRole name: monitoring-endpoints-role
<.> Required permission.
<.> ClusterRoleBinding name: monitoring-role-binding

Execute the below command to create rbac in the cluster

[source,console]

----
$ kubectl apply -f "rbac.yaml"
----

==== Monitoring Dashboard

Now we need to provide the actual CMOS deployment to run along with the supporting services for it. Save the below content in `monitoring-dashboard.yaml`.

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
    name: couchbase-grafana # <.>
spec:
    selector:
        matchLabels:
            run: couchbase-grafana
    replicas: 1
    template:
        metadata:
            labels:
                run: couchbase-grafana
        spec:
            containers:
                - name: couchbase-grafana
                  image: couchbase/observability-stack:v1 # <.>
                  ports:
                      - name: http
                        containerPort: 8080
                      - name: loki # So we can push logs to it
                        containerPort: 3100
                  env:
                      - name: KUBERNETES_DEPLOYMENT
                        value: 'true'
                      - name: ENABLE_LOG_TO_FILE
                        value: 'true'
                      - name: PROMETHEUS_CONFIG_FILE
                        value: /etc/prometheus/custom/prometheus-k8s.yml
                      - name: PROMETHEUS_CONFIG_TEMPLATE_FILE
                        value: ignore
                  volumeMounts:
                      - name: prometheus-config-volume
                        mountPath: /etc/prometheus/custom # keep /etc/prometheus for any defaults
      # Now we watch for changes to the volumes and auto-reload the prometheus configuration if seen
                - name: prometheus-config-watcher
                  image: weaveworks/watch:master-9199bf5
                  args: [-v, -t, -p=/etc/prometheus/custom, curl, -X, POST, --fail, -o, '-', -sS, http://localhost:8080/prometheus/-/reload]
                  volumeMounts:
                      - name: prometheus-config-volume
                        mountPath: /etc/prometheus/custom
            volumes:
                - name: prometheus-config-volume
                  configMap:
                      name: prometheus-config
---
apiVersion: v1
kind: Service
metadata:
    name: couchbase-grafana-http
    labels:
        run: couchbase-grafana
spec:
    ports:
        - port: 8080
          protocol: TCP
    selector:
        run: couchbase-grafana
---
# To allow us to send to Loki we need this
apiVersion: v1
kind: Service
metadata:
    name: loki
    labels:
        run: couchbase-grafana
spec:
    ports:
        - port: 3100
          protocol: TCP
    selector:
        run: couchbase-grafana
----

<.> Deployment name of observability dashboard: couchbase-grafana
<.> Image of observability dashboard: couchbase/observability-stack:v1

Execute the below command to create monitoring dashboard in the cluster
[source,console]
----
$ kubectl apply -f "monitoring-dashboard.yaml"
----

=== Fluent Bit

For setting up the Fluent Bit, Save the below content in a file called `fluent-bit.conf`
[source,console]
----
@include /fluent-bit/etc/fluent-bit.conf

# The following needs a 'loki' host
@include /fluent-bit/etc/couchbase/out-loki.conf
----

Create the fluent-bit secret.

[NOTE]
====
Before creating, We need to first delete the existing `fluent-bit-custom` secret if exists. 
====

[source,console]
----
$ kubectl delete secret fluent-bit-custom
----
Run the below command to create a fluent-bit secret.
[source,console]
----
$ kubectl create secret generic fluent-bit-custom --from-file="fluent-bit.conf"
----

==== verify

Verify the created fluent bit secret. The output of below command should match with the content of `fluent-bit.conf` file.
[source,console]
----
kubectl get secret fluent-bit-custom -o go-template='{{range $k,$v := .data}}{{printf "%s: " $k}}{{if not $v}}{{$v}}{{else}}{{$v | base64decode}}{{end}}{{"\n"}}{{end}}'
----

== Deploy Couchbase

The https://docs.couchbase.com/operator/current/helm-setup-guide.html[Couchbase Helm chart^] is used to deploy CAO and a default configuration for the Couchbase Server pods.

This is a very simple process to use defaults, to modify any of the values to configure it for your needs please refer to the https://docs.couchbase.com/operator/current/helm-setup-guide.html#custom-installation[Helm documentation^].

We provide an example to show how to set up https://docs.couchbase.com/operator/current/howto-couchbase-log-forwarding.html[log forwarding^] simply to CMOS via Kubernetes annotations on the pod.

[source,yaml]
----
---
cluster:
    logging:
        server:
            enabled: true
            sidecar:
                image: couchbase/fluent-bit:1.1.2
    monitoring:
        prometheus:
            enabled: false # We're using server 7 metrics directly
    security:
        # To simplify for scraping we hardcode the passwords.
        # They should be auto-generated really and the Secret value
        # used by mounting into CMOS (or getting it to retrieve during
        # startup).
        username: admin
        password: password
    servers:
        # We use custom annotations to forward to CMOS Loki
        default:
            size: 3
            pod:
                metadata:
                    annotations:
                        # Match all logs
                        fluentbit.couchbase.com/loki_match: "*"
                        # Send to this SVC
                        fluentbit.couchbase.com/loki_host: loki.default
            volumeMounts:
                default: couchbase
    volumeClaimTemplates:
        - metadata:
              name: couchbase
          spec:
              resources:
                  requests:
                      storage: 1Gi
----

With the configuration above saved in a `custom-values.yaml` file, we can deploy using a nominated Couchbase Server version very easily.

[source,console]
----
$ helm upgrade --install couchbase couchbase/couchbase-operator --set cluster.image=couchbase/server:7.0.2 --values=custom-values.yaml
----

This will deploy the entire Couchbase operator and server set of pods all fully managed.

== Deploy Ingress

To allow us to access the cluster we set up a Kubernetes `Ingress` to forward traffic from our localhost to the appropriate parts of the cluster.
There are two aspects required here:
. Provide an Ingress controller, Nginx in this case.
. Set up Ingress to forward to our CMOS deployment.

For a production system it is likely an `Ingress` controller will already be deployed.
The example also just forwards all traffic whereas a non-local cluster really should just target CMOS traffic as other traffic likely has `Ingress` rules already.

We follow the https://kind.sigs.k8s.io/docs/user/ingress/#ingress-nginx[KIND documentation^] to deploy Nginx.

[source,console]
----
$ INGRESS_VERSION=$(curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/stable.txt)
$ kubectl apply -f "https://raw.githubusercontent.com/kubernetes/ingress-nginx/${INGRESS_VERSION}/deploy/static/provider/kind/deploy.yaml"
$ kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=120s
----

The last step deploys the actual `Ingress` configuration as below.

[source,yaml]
----
---
# Ingress to forward to our web server including sub-paths: we should just forward what we need but for local testing just sending it all.
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
    name: couchbase-ingress
    annotations:
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/rewrite-target: /$1$2
        nginx.ingress.kubernetes.io/use-regex: 'true'
spec:
    rules:
        - http:
              paths:
                  - path: /(.*)(/|$)
                    pathType: Prefix
                    backend:
                        service:
                            name: couchbase-grafana-http
                            port:
                                number: 8080
---
----

[source,console]
----
$ kubectl apply -f ingress.yaml
----

== Using CMOS

Once this is all deployed, which can be automated using a https://github.com/couchbaselabs/observability/blob/main/examples/kubernetes/run.sh[fully scripted version^], then you should be able to browse to http://localhost (or wherever you are deploying it with the appropriate `Ingress`).

You should then hit the landing page which includes links to this documentation.

You can verify all the Prometheus targets are set up.

.Successful Prometheus discovery
image::prometheus-successful-targets.png[]

Whilst the pods are coming up, some may report as failing but these will resolve once the pods are running.

.Failing targets during startup
image::prometheus-failing-targets.png[]

=== CMOS cluster configuration

[NOTE]
====
Unfortunately cluster monitor is currently designed for an on-premise solution so will not cope with auto-discovery on Kubernetes yet.
Without this the various Grafana dashboards will not function correctly so you have to do it manually for now.
====

Kubernetes support is on the roadmap but, in the meantime, to manually add the cluster details you will need the credentials for accessing the REST API and a pod name for one of the Couchbase Server pods in the cluster (it will bootstrap the rest).

These can then be added via the `Add Cluster` link on the landing page.
Make sure to disable the `Add Nodes to Prometheus` option as this is handled already.

.Add cluster screenshot
image::add-cluster-example.png[]

Once this is added then the other Grafana dashboards should spring to life.

=== Grafana

All the components are already configured and linked while setup. So the metrics can be accessible on respective endpoints.

==== Couchbase Exporter metrics

The couchbase Exporter Metrics is accssible on `http://localhost/grafana/d/Au2tCFknk/couchbase-exporter`

.Couchbase Exporter Metrics screenshot
image::couchbase-exporter.png[]


==== Couchbase Kubernetes metrics

The couchbase Exporter Metrics is accssible on `http://localhost/grafana/d/QpALMugZz/couchbase-kubernetes`

.Couchbase Kubernetes Metrics screenshot
image::couchbase-kubernetes.png[]

==== Alerting

Alerting rule is configured can be seen on `http://localhost/grafana/d/eea-9_sik/prometheus-alerts`

.Prometheus Alerting screenshot
image::prometheus-alert.png[]