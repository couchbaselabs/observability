= Kubernetes Deployment

include::partial$tutorial.adoc[]

== Overview

CMOS is a simple, out-of-the-box solution based on industry standard tooling to observe the state of your Couchbase cluster. You can refer to xref:architecture.adoc[Architecture overview] section to learn more.

CMOS can be deployed to monitor the Couchbase clusters deployed via xref:operator:overview.adoc[Couchbase Autonomous Operator] on a kubernetes cluster.

== Deployment

CMOS is deployed on Kubernetes platform using a standard set of resources like Deployment, Services etc. Following sections describe how to deploy these standard objects. They also include information on configuring these services.

=== Kube State Metrics

A prerequisite to configuring CMOS is to install https://github.com/kubernetes/kube-state-metrics[kube-state-metrics^]. If not already installed you can install it using the following commands.

[source,console]
----
$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
$ helm repo update
$ helm upgrade --install kube-state-metrics prometheus-community/kube-state-metrics
----

=== Prometheus Configuration

We provide Prometheus configuration via a Kubernetes `ConfigMap` which contains all the details for credentials, scrape targets, etc. The prometheus configuration file is a standard way of configuring prometheus server.

Below is an example configuration file content which has default configuration for CMOS prometheus for it to work out of the box. Save the contents to a file `prometheus-config.yaml`.

[source,yaml]

----
# This is a template file we use so we can substitute environment variables at launch
global:
    scrape_interval: 30s
    evaluation_interval: 30s
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
    external_labels:
        monitor: couchbase-observability-stack

# Load and evaluate rules in this file every 'evaluation_interval' seconds.
rule_files: # <.>
  # All Couchbase default rules go here
    - /etc/prometheus/alerting/couchbase/*.yaml
    - /etc/prometheus/alerting/couchbase/*.yml
  # All custom rules can go here: relative to this file
    - alerting/*.yaml
    - alerting/*.yml

alerting: # <.>
    alertmanagers:
        - scheme: http
    # tls_config:
    #   ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    # bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          path_prefix: /alertmanager/
    # Assumption is we always have AlertManager with Prometheus
          static_configs:
              - targets:
                    - localhost:9093
    # Discover alert manager instances using K8s service discovery
    # kubernetes_sd_configs:
    #   - role: pod
    # relabel_configs:
    # - source_labels: [__meta_kubernetes_namespace]
    #   regex: monitoring
    #   action: keep
    # - source_labels: [__meta_kubernetes_pod_label_app]
    #   regex: prometheus
    #   action: keep
    # - source_labels: [__meta_kubernetes_pod_label_component]
    #   regex: alertmanager
    #   action: keep
    # - source_labels: [__meta_kubernetes_pod_container_port_number]
    #   regex:
    #   action: drop

scrape_configs: # <.>
    - job_name: prometheus
      metrics_path: /prometheus/metrics
      static_configs:
          - targets: [localhost:9090]

    - job_name: couchbase-grafana
      file_sd_configs:
          - files:
                - /etc/prometheus/couchbase/monitoring/*.json
            refresh_interval: 30s

  # TODO: add unauthenticated endpoint
    - job_name: couchbase-cluster-monitor
      basic_auth:
          username: admin
          password: password
      metrics_path: /api/v1/_prometheus
    # For basic auth we cannot use file_sd
      static_configs:
          - targets: [localhost:7196]

  # Used for kubernetes deployment as we can discover the end points to scrape from the API
    - job_name: couchbase-kubernetes-pods
      kubernetes_sd_configs:
          - role: pod
      relabel_configs:
      # Scrape pods labeled with app=couchbase and then only port 9091 (exporter) or 2020 (fluent bit)
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: couchbase
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            action: keep
            regex: (9091|2020)
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

  # Kube-state-metrics default service to scrape
    - job_name: kube-state-metrics
      static_configs:
          - targets: [kube-state-metrics:8080]
----

<.> `rule_files`: Promethues is configured to load rules via rule_files. You can extend the rules by adding rule files under `alerting/` directory. Note that the `alerting/` directory is a relative path to the prometheus configuration file. By default the complete path is `/etc/prometheus/custom/alerting/`. Refer to the [Monitoring Dashboard] section for volume mounts.
<.> `alerting`: Alert manager is shipped and enabled by default in the CMOS. This section has various configurations of alert manager.
<.> `scrape_configs`: All the targets to scrape metrics are defined here. This includes prometheus, couchbase-grafana, couchbase cluster, couchbase-kubernetes-pods and kube-state-metrics. We try to discover the couchbase pods using labels.

Run the below command to create the prometheus config.

[source,console]

----
kubectl create configmap prometheus-config-cmos --from-file=prometheus-config.yaml
----

=== Observability Stack

==== RBAC

Kubernetes controls access to its reource via RBAC. The CMOS deployment requires communication with couchbase cluster and its discovery in order to monitor it. Save the contents of below section in `rbac.yaml`.
[source, yaml]
----
# Prometheus service discovery via the K8S API requires some permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
    name: monitoring-endpoints-role
    labels:
        rbac.couchbase.observability.com/aggregate-to-monitoring: 'true'
rules:
    - apiGroups: [''] # <.>
      resources: [services, endpoints, pods, secrets] 
      verbs: [get, list, watch]
    - apiGroups: [couchbase.com] # <.>
      resources: [couchbaseclusters]
      verbs: [get, list, watch]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
    name: monitoring-role-binding <.>
roleRef:
    kind: ClusterRole
    name: monitoring-endpoints-role
    apiGroup: rbac.authorization.k8s.io
subjects:
    - kind: Group
      name: system:serviceaccounts
      apiGroup: rbac.authorization.k8s.io
----

By taking a quick look at this configuration file, you can see that it defines a cluster role by specifying the following permissions.

<.> Access to standard Kubernetes resources: CMOS requires get, list and watch permissions to `services, endpoints, pods, secrets` resources.
<.> Couchbase Custom Resource Definition: CMOS requires get, list and watch permissions to `couchbaseclusters` resource.
<.> `monitoring-role-binding`: This role binding is required to give the permissions created in ClusterRole to service account of CMOS.

Execute the below command to create the roles in the cluster

[source,console]

----
$ kubectl apply -f rbac.yaml
----

==== CMOS Workload

The actual CMOS workload runs as a kubernetes deployment along with other supporting services. Save the below content in `cmos-workload.yaml`.

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
    name: couchbase-grafana
spec:
    selector:
        matchLabels:
            run: couchbase-grafana
    replicas: 1
    template:
        metadata:
            labels:
                run: couchbase-grafana
        spec:
            containers:
                - name: couchbase-grafana
                  image: couchbase/observability-stack:v1 # <.>
                  ports:
                      - name: http
                        containerPort: 8080
                      - name: loki # So we can push logs to it
                        containerPort: 3100
                  env:
                      - name: KUBERNETES_DEPLOYMENT
                        value: 'true'
                      - name: ENABLE_LOG_TO_FILE <.>
                        value: 'true'
                      - name: PROMETHEUS_CONFIG_FILE
                        value: /etc/prometheus/custom/prometheus-config.yaml
                      - name: PROMETHEUS_CONFIG_TEMPLATE_FILE
                        value: ignore
                    # - name: DISABLE_LOKI <.>
                    #   value: "true"
                  volumeMounts:
                      - name: prometheus-config-volume
                        mountPath: /etc/prometheus/custom # keep /etc/prometheus for any defaults
      # Now we watch for changes to the volumes and auto-reload the prometheus configuration if seen
                - name: prometheus-config-watcher
                  image: weaveworks/watch:master-9199bf5
                  args: [-v, -t, -p=/etc/prometheus/custom, curl, -X, POST, --fail, -o, '-', -sS, http://localhost:8080/prometheus/-/reload]
                  volumeMounts:
                      - name: prometheus-config-volume
                        mountPath: /etc/prometheus/custom
            volumes:
                - name: prometheus-config-volume
                  configMap:
                      name: prometheus-config-cmos <.>
----

<.> `spec.template.spec.containers[0].image`: This is the CMOS image that we wish to run.
<.> Refer to the logging section for more information
<.> Refer to the disabling services section for more information

Execute the below command to deploy cmos in the cluster
[source,console]
----
$ kubectl apply -f cmos-workload.yaml
----

==== Disabling Service
By default all the service of CMOS stack are enabled. You can choose to disable a service from being run, this is achieved by setting environment variables of the container. The naming convention to be followed to disable a service is `DISABLE_X`, where X is the name of the service.
[source, console]
----
# To disable Alert Manager
DISABLE_ALERT_MANAGER

# To disable Grafana
DISABLE_GRAFANA

# To disable Jaeger
DISABLE_JAEGER

# To disable Loki
DISABLE_LOKI

# To disable Prometheus
DISABLE_PROMETHEUS

# To disable Webserver (The nginx proxy to serve landing page and other components)
DISABLE_WEBSERVER
----

==== Logging
Logs can be either provided to `stdout` by default or to service-based logs by setting `ENABLE_LOG_TO_FILE`. With latter logs will appear into the `/logs/` directory. You can further expose the logs externally via https://kubernetes.io/docs/concepts/storage/volumes/[kubernetes volume^].

==== Kubernetes Services
Once the observability dashboard is deployed then we need to create a service to access the dashboard. Save the below content in a file called `monitoring-service.yaml`

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
    name: couchbase-grafana-http
    labels:
        run: couchbase-grafana
spec:
    ports:
        - port: 8080 # <.>
          protocol: TCP
    selector:
        run: couchbase-grafana
----

<.> The observability monitoring service is running on port 8080

Execute the below command to create monitoring service in the cluster.

[source,console]
----
$ kubectl apply -f monitoring-service.yaml
----

Create a service for accessing the Loki, save the below content in a file called `loki-service.yaml`

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
    name: loki
    labels:
        run: couchbase-grafana
spec:
    ports:
        - port: 3100
          protocol: TCP
    selector:
        run: couchbase-grafana
----

Execute the below command to create Loki service in the cluster.

[source,console]
----
$ kubectl apply -f loki-service.yaml
----

=== Fluent Bit

Setup the Fluent Bit, which is used for collecting the logs. Save the below content in a file called `fluent-bit.conf`

[source,console]
----
@include /fluent-bit/etc/fluent-bit.conf

# The following needs a 'loki' host
@include /fluent-bit/etc/couchbase/out-loki.conf
----

Execute the below command to create fluent bit secret in the cluster.

[source,console]
----
$ kubectl create secret generic fluent-bit-custom --from-file=fluent-bit.conf
----

==== Verify

Verify the created fluent bit secret. The output of below command should match with the content of `fluent-bit.conf` file.
[source,console]
----
kubectl get secret fluent-bit-custom -o go-template='{{range $k,$v := .data}}{{printf "%s: " $k}}{{if not $v}}{{$v}}{{else}}{{$v | base64decode}}{{end}}{{"\n"}}{{end}}'
----

== Configure Couchbase

The https://docs.couchbase.com/operator/current/helm-setup-guide.html[Couchbase Helm chart^] is used to deploy Couchbase Autonomous Operator and a default configuration for the Couchbase Server pods.

CMOS requires the operator installation with certain configuration. This configuration enables the integration of CMOS with Couchbase clusters. Save the content below in a file called `custom-values.yaml`.

[source,yaml]
----
---
cluster:
    logging:
        server:
            enabled: true
            sidecar:
                image: couchbase/fluent-bit:1.1.2
    monitoring:
        prometheus:
            enabled: false # We're using server 7 metrics directly
    security:
        # To simplify for scraping we hardcode the passwords.
        # They should be auto-generated really and the Secret value
        # used by mounting into CMOS (or getting it to retrieve during
        # startup).
        username: admin
        password: password
    servers:
        # We use custom annotations to forward to CMOS Loki
        default:
            size: 3
            pod:
                metadata:
                    annotations:
                        # Match all logs
                        fluentbit.couchbase.com/loki_match: "*"
                        # Send to this SVC
                        fluentbit.couchbase.com/loki_host: loki.default
            volumeMounts:
                default: couchbase
    volumeClaimTemplates:
        - metadata:
              name: couchbase
          spec:
              resources:
                  requests:
                      storage: 1Gi
----

You can learn more about above configuration on https://docs.couchbase.com/operator/current/helm-setup-guide.html#custom-installation[Helm documentation^] page.

[NOTE]
====
If you have the couchbase operator already deployed using helm or deploying new couchbase operator then below command can be used with custom values to enable the CMOS. If it is deployed through command line tools then you need to update the existing service through https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/[kubectl patch^] with custom values mentioned above.
====

The command below tries to upgrade the existing version of deployed couchbase operator. If the operator is not already installed it will install it.
[WARNING]
====
Please be extremely cautious when upgrading an installed version of operator. Misconfigured `custom-values.yaml` can cause issues in the operator installation.
====
[source,console]
----
$ helm upgrade --install couchbase couchbase/couchbase-operator --set cluster.image=couchbase/server:7.0.2 --values=custom-values.yaml
----


== Accessing CMOS

=== Deploying Ingress
To allow us to access the cluster we set up a Kubernetes `Ingress` to forward traffic from our `localhost` to the appropriate parts of the cluster.
There are two aspects required here:
. Provide an Ingress controller, Nginx in this case.
. Set up Ingress to forward to our CMOS deployment.

For a production system it is likely an `Ingress` controller will already be deployed.
The example also just forwards all traffic whereas a non-local cluster really should just target CMOS traffic as other traffic likely has `Ingress` rules already.

Follow this https://kubernetes.github.io/ingress-nginx/deploy/[Nginx Ingress Controller^] guideline to setup it.

Once the Ingress controller is installed and ready then the last step deploys the actual `Ingress` configuration as below. Save the below content in a file called `ingress.yaml`

[source,yaml]
----
---
# Ingress to forward to our web server including sub-paths: we should just forward what we need but for local testing just sending it all.
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
    name: couchbase-ingress
    annotations:
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/rewrite-target: /$1$2
        nginx.ingress.kubernetes.io/use-regex: 'true'
spec:
    rules:
        - http:
              paths:
                  - path: /(.*)(/|$)
                    pathType: Prefix
                    backend:
                        service:
                            name: couchbase-grafana-http
                            port:
                                number: 8080
---
----

Execute the below command to create ingress in the cluster.

[source,console]
----
$ kubectl apply -f ingress.yaml
----

== Accessing CMOS

Once this is all deployed then you should be able to browse to http://localhost (or wherever you are deploying it with the appropriate `Ingress`).

You should then hit the landing page which includes links to documentation, cluster manager service and various other services of CMOS.

=== Verify Prometheus Targets
You can verify all the Prometheus targets are set up.

.Successful Prometheus discovery
image::prometheus-successful-targets.png[]

Whilst the pods are coming up, some may report as failing but these will resolve once the pods are running.

.Failing targets during startup
image::prometheus-failing-targets.png[]

=== CMOS cluster configuration

[NOTE]
====
Unfortunately cluster monitor is currently designed for an on-premise solution so will not cope with auto-discovery on Kubernetes yet.
Without this the various Grafana dashboards will not function correctly so you have to do it manually for now.
====

Kubernetes support is on the roadmap but, in the meantime, to manually add the cluster details you will need the credentials for accessing the REST API and a pod name for one of the Couchbase Server pods in the cluster (it will bootstrap the rest).

These can then be added via the `Add Cluster` link on the landing page.
Make sure to disable the `Add Nodes to Prometheus` option as this is handled already.

.Add cluster screenshot
image::add-cluster-example.png[]

Once this is added then the other Grafana dashboards should spring to life.

=== Grafana

Various dashboard to monitor couchbase cluster is shipped out of the box in Grafana. Connect a couchbase cluster and navigate to Grafana to see the graphs. You can list all the dashboards using the https://grafana.com/docs/grafana/latest/dashboards/search/#dashboard-search[search dashboard option^]. You may create https://grafana.com/docs/grafana/latest/getting-started/getting-started/#step-3-create-a-dashboard[additional dashboards^] as per your needs. Post successful completion of the setup you should be able to see metrics in your grafana dashboard:
Following are some dashboards readily available.

==== Couchbase Exporter metrics

Couchbase Exporter Metrics dashboard can be accessible on Grafana with name as: `couchbase-exporter`

.Couchbase Exporter Metrics screenshot
image::couchbase-exporter.png[]

==== Couchbase Kubernetes metrics

Couchbase Kubernetes Metrics dashboard can be accessible on Grafana with name as: `couchbase-kubernetes`

.Couchbase Kubernetes Metrics screenshot
image::couchbase-kubernetes.png[]

==== Alerting

While setting up the prometheus config alerting is also getting configured, refer <<alert_rules>> to see the configuration

Alerting dashboard can be accessible on Grafana with name as: `prometheus-alerts`

.Prometheus Alerting screenshot
image::prometheus-alert.png[]

=== Alert Manager
TODO: Add screenshot of alert manager
